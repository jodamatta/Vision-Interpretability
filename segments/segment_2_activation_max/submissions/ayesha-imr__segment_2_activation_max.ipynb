{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 02: Activation Maximization\n",
    "\n",
    "**Goal**: Understand what neurons in a neural network \"see\" by generating images that maximally activate them.\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "Normally we ask: *\"Given this image, what does the network predict?\"*\n",
    "\n",
    "Activation maximization flips this: *\"What image would make this specific neuron most excited?\"*\n",
    "\n",
    "We do this by:\n",
    "1. Starting with random noise\n",
    "2. Freezing the network weights\n",
    "3. Using gradient descent to modify the **pixels** (not weights!) to maximize a neuron's activation\n",
    "\n",
    "The resulting image reveals what pattern that neuron has learned to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "from lucent.modelzoo import inceptionv1\n",
    "\n",
    "# Load pretrained InceptionV1 (GoogLeNet)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = inceptionv1(pretrained=True).to(device).eval()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Visualizing the First 10 Neurons of `mixed4a`\n",
    "\n",
    "The `mixed4a` layer is a middle layer in InceptionV1. Neurons here typically respond to textures, patterns, and simple object parts — more complex than edges but simpler than whole objects.\n",
    "\n",
    "Let's generate activation maximization images for channels 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations for channels 0-9 of mixed4a\n",
    "layer = \"mixed4a\"\n",
    "num_channels = 10\n",
    "\n",
    "visualizations = []\n",
    "for ch in range(num_channels):\n",
    "    print(f\"Generating visualization for {layer}:{ch}\")\n",
    "    imgs = render.render_vis(model, f\"{layer}:{ch}\", show_inline=False, thresholds=(256,))\n",
    "    visualizations.append(imgs[0][0])  # Get the final image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all 10 visualizations in a grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, img) in enumerate(zip(axes, visualizations)):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{layer}:{i}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Activation Maximization: First 10 channels of {layer}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Look at the generated images above. Each one shows what that particular neuron \"wants to see\":\n",
    "\n",
    "- Some neurons might detect **textures** (fur, scales, patterns)\n",
    "- Some might detect **colors** or color gradients\n",
    "- Some might detect **shapes** or curves\n",
    "- Some might be harder to interpret\n",
    "\n",
    "*Question to ponder*: Do any of these patterns remind you of real-world objects or textures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Effect of Parameterization\n",
    "\n",
    "The \"parameterization\" is *how* we represent the image during optimization. This choice dramatically affects interpretability.\n",
    "\n",
    "Let's compare different parameterizations on a single neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one channel to study in detail\n",
    "test_channel = 5\n",
    "obj = objectives.channel(layer, test_channel)\n",
    "\n",
    "# Different parameterization settings\n",
    "param_configs = [\n",
    "    {\"name\": \"Pixel (naive)\", \"fft\": False, \"decorrelate\": False},\n",
    "    {\"name\": \"FFT only\", \"fft\": True, \"decorrelate\": False},\n",
    "    {\"name\": \"Decorrelate only\", \"fft\": False, \"decorrelate\": True},\n",
    "    {\"name\": \"FFT + Decorrelate (default)\", \"fft\": True, \"decorrelate\": True},\n",
    "]\n",
    "\n",
    "param_results = []\n",
    "for cfg in param_configs:\n",
    "    print(f\"Testing: {cfg['name']}\")\n",
    "    param_f = lambda fft=cfg[\"fft\"], dec=cfg[\"decorrelate\"]: param.image(128, fft=fft, decorrelate=dec)\n",
    "    imgs = render.render_vis(model, obj, param_f, transforms=[], show_inline=False, thresholds=(256,))\n",
    "    param_results.append((cfg[\"name\"], imgs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameterizations\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, (name, img) in zip(axes, param_results):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(name)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Effect of Parameterization on {layer}:{test_channel}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening here?\n",
    "\n",
    "| Parameterization | What it does | Visual effect |\n",
    "|-----------------|--------------|---------------|\n",
    "| **Pixel (naive)** | Optimizes raw pixels directly | High-frequency noise (adversarial-like) |\n",
    "| **FFT** | Optimizes in frequency domain with 1/f scaling | Smoother, prefers low frequencies |\n",
    "| **Decorrelate** | Transforms to independent color space | More natural color combinations |\n",
    "| **FFT + Decorrelate** | Both | Most interpretable results |\n",
    "\n",
    "The FFT parameterization encodes a \"prior\" that natural images have mostly low frequencies. Without it, the optimizer finds high-frequency patterns that activate the neuron but look like noise to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Effect of Transforms\n",
    "\n",
    "Transforms are augmentations applied *during* optimization. They prevent the optimizer from finding position-specific tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without transforms\n",
    "transform_configs = [\n",
    "    {\"name\": \"No transforms\", \"transforms\": []},\n",
    "    {\"name\": \"Jitter only\", \"transforms\": [transform.jitter(8)]},\n",
    "    {\"name\": \"Standard transforms\", \"transforms\": transform.standard_transforms},\n",
    "]\n",
    "\n",
    "transform_results = []\n",
    "for cfg in transform_configs:\n",
    "    print(f\"Testing: {cfg['name']}\")\n",
    "    imgs = render.render_vis(model, obj, transforms=cfg[\"transforms\"], show_inline=False, thresholds=(256,))\n",
    "    transform_results.append((cfg[\"name\"], imgs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transforms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for ax, (name, img) in zip(axes, transform_results):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(name)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Effect of Transforms on {layer}:{test_channel}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why transforms matter\n",
    "\n",
    "Without transforms, the optimizer might learn: *\"put pattern X at exactly pixel (50, 50)\"*.\n",
    "\n",
    "With jitter/rotation/scale, it must find patterns that work at *any* position — revealing more generalizable features the neuron detects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comparing Layers\n",
    "\n",
    "Let's see how features change across network depth. Earlier layers detect simpler patterns; later layers detect more complex, abstract concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare same channel index across different layers\n",
    "layers_to_compare = [\"conv2d2\", \"mixed3a\", \"mixed4a\", \"mixed5a\"]\n",
    "channel_idx = 3  # Arbitrary channel\n",
    "\n",
    "layer_results = []\n",
    "for lyr in layers_to_compare:\n",
    "    print(f\"Visualizing {lyr}:{channel_idx}\")\n",
    "    imgs = render.render_vis(model, f\"{lyr}:{channel_idx}\", show_inline=False, thresholds=(256,))\n",
    "    layer_results.append((lyr, imgs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare across layers\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, (lyr, img) in zip(axes, layer_results):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{lyr}:{channel_idx}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Features Across Network Depth (early → late)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "- **Early layers** (conv2d2): Simple edges, colors, Gabor-like filters\n",
    "- **Middle layers** (mixed3a, mixed4a): Textures, patterns, object parts\n",
    "- **Late layers** (mixed5a): More complex, sometimes recognizable objects\n",
    "\n",
    "This hierarchy — from simple to complex — emerges naturally from training on images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What Did We Learn?\n",
    "\n",
    "1. **Activation maximization** generates images that maximally activate specific neurons, revealing what they've learned to detect.\n",
    "\n",
    "2. **Parameterization matters**: FFT + color decorrelation produces interpretable images by encoding priors about natural image statistics.\n",
    "\n",
    "3. **Transforms matter**: Random augmentations during optimization reveal position-invariant features.\n",
    "\n",
    "4. **Depth matters**: Features progress from simple (edges) to complex (objects) through the network.\n",
    "\n",
    "### Limitations to keep in mind\n",
    "\n",
    "- These visualizations show what *maximally* activates a neuron, not necessarily what it responds to in real images\n",
    "- Some neurons may be \"polysemantic\" — responding to multiple unrelated concepts\n",
    "- The parameterization choices affect what we see; different priors reveal different aspects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
